# åˆ†å¸ƒå¼è®­ç»ƒå®éªŒ - å¿«é€Ÿæ“ä½œæŒ‡å—

## ğŸ¯ ä¸‰æ­¥å®Œæˆæ‰€æœ‰å®éªŒ

### æ–¹æ³•1ï¼šè‡ªåŠ¨åŒ–è¿è¡Œï¼ˆæ¨èï¼‰

```bash
# ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
chmod +x run_all_experiments.sh

# ä¸€é”®è¿è¡Œæ‰€æœ‰å®éªŒ
./run_all_experiments.sh
```

è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨ï¼š
1. æ¸…ç†æ—§ç»“æœ
2. ä¾æ¬¡è¿è¡Œä¸‰ä¸ªå®éªŒï¼ˆBaseline DDPã€All-Reduceã€Parameter Serverï¼‰
3. ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨

**é¢„è®¡æ€»æ—¶é—´:** 30-90åˆ†é’Ÿï¼ˆå–å†³äºæ•°æ®é›†å¤§å°å’Œç¡¬ä»¶æ€§èƒ½ï¼‰

---

### æ–¹æ³•2ï¼šæ‰‹åŠ¨åˆ†æ­¥è¿è¡Œ

å¦‚æœéœ€è¦æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥æ‰‹åŠ¨è¿è¡Œæ¯ä¸ªå®éªŒï¼š

#### ç¬¬1æ­¥ï¼šè¿è¡ŒBaseline DDP
```bash
torchrun --nproc_per_node=4 baseline_multi_card.py
```

#### ç¬¬2æ­¥ï¼šè¿è¡ŒManual All-Reduce
```bash
torchrun --nproc_per_node=4 all_reduce_train.py --epochs 3 --batch-size 64 --workers 16 --data-dir ./ --backend nccl
```

#### ç¬¬3æ­¥ï¼šè¿è¡ŒParameter Server
```bash
torchrun --nproc_per_node=4 ps_train.py --epochs 3 --batch-size 64 --workers 16 --data-dir ./ --backend nccl
```

#### ç¬¬4æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š
```bash
python analyze_results.py
```

---

## ğŸ“Š æŸ¥çœ‹ç»“æœ

### å®éªŒè¾“å‡ºæ–‡ä»¶

å®éªŒå®Œæˆåä¼šç”Ÿæˆï¼š

**1. JSONç»“æœæ–‡ä»¶ï¼ˆåŸå§‹æ•°æ®ï¼‰:**
- `results_baseline_ddp.json` - Baseline DDPçš„è¯¦ç»†æŒ‡æ ‡
- `results_all_reduce.json` - Manual All-Reduceçš„è¯¦ç»†æŒ‡æ ‡
- `results_ps.json` - Parameter Serverçš„è¯¦ç»†æŒ‡æ ‡

**2. æ€§èƒ½æŠ¥å‘Š:**
- `performance_report.txt` - æ–‡æœ¬æ ¼å¼çš„è¯¦ç»†æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š

**3. å¯è§†åŒ–å›¾è¡¨ï¼ˆ./plots/ ç›®å½•ï¼‰:**
- `throughput_comparison.png` - ååé‡å¯¹æ¯”æŸ±çŠ¶å›¾
- `training_time_comparison.png` - è®­ç»ƒæ—¶é—´å¯¹æ¯”æŸ±çŠ¶å›¾
- `loss_curves.png` - Lossæ”¶æ•›æ›²çº¿
- `accuracy_curves.png` - å‡†ç¡®ç‡æ›²çº¿
- `communication_overhead.png` - é€šä¿¡å¼€é”€å¯¹æ¯”

### å¿«é€ŸæŸ¥çœ‹ç»“æœ

```bash
# æŸ¥çœ‹æ€§èƒ½å¯¹æ¯”è¡¨æ ¼ï¼ˆç»ˆç«¯è¾“å‡ºï¼‰
cat performance_report.txt | head -50

# æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨
ls -lh plots/

# åœ¨VS Codeä¸­é¢„è§ˆå›¾è¡¨
code plots/throughput_comparison.png
```

---

## ğŸ“ æ’°å†™æŠ¥å‘Š

### Section 3.2 (Implementation) - éœ€è¦çš„å†…å®¹

**Parameter Serverå®ç°åˆ†æ:**

1. **æ¶æ„è¯´æ˜:**
   - Rank 0 = Parameter Serverï¼ˆå‚æ•°æœåŠ¡å™¨ï¼‰
   - Rank 1-3 = Workersï¼ˆå·¥ä½œè¿›ç¨‹ï¼‰
   - åŒæ­¥è®­ç»ƒæ¨¡å¼

2. **å…³é”®ä»£ç ä½ç½®ï¼ˆps_train.pyï¼‰:**
   - è§’è‰²åˆ†é…ï¼šç¬¬20è¡Œ
   - å‚æ•°ä¸‹è½½ï¼šç¬¬120-122è¡Œï¼ˆWorkersä»PSæ‹‰å–å‚æ•°ï¼‰
   - æ¢¯åº¦ä¸Šä¼ ï¼šç¬¬136-138è¡Œï¼ˆWorkersæ¨é€æ¢¯åº¦ç»™PSï¼‰
   - å‚æ•°å¹¿æ’­ï¼šç¬¬178-181è¡Œï¼ˆPSåˆ†å‘å‚æ•°ç»™Workersï¼‰
   - æ¢¯åº¦èšåˆï¼šç¬¬184-193è¡Œï¼ˆPSæ¥æ”¶å¹¶å¹³å‡æ¢¯åº¦ï¼‰

3. **é€šä¿¡æµç¨‹å›¾:**
```
æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡:
PS ---[broadcast params]---> Workers (æ‰€æœ‰Worker)
Workers --[forward + backward]-->
Workers ---[send gradients]---> PS (é€ä¸ªWorker)
PS --[aggregate & update]-->
é‡å¤
```

4. **ä¸DDPçš„åŒºåˆ«:**
   - DDP: ä½¿ç”¨All-Reduceï¼Œæ‰€æœ‰è¿›ç¨‹å¯¹ç­‰
   - PS: ä¸­å¿ƒåŒ–æ¶æ„ï¼ŒPSæ˜¯ç“¶é¢ˆç‚¹
   - DDPé€šä¿¡æ›´é«˜æ•ˆï¼ˆæ ‘å½¢æ‹“æ‰‘ã€NCCLä¼˜åŒ–ï¼‰

---

### Section 4.2 (Evaluation) - éœ€è¦çš„å†…å®¹

**ä» `performance_report.txt` ä¸­æå–å…³é”®æ•°æ®:**

1. **æ€§èƒ½å¯¹æ¯”è¡¨æ ¼:**

| æ–¹æ³• | å¹³å‡ååé‡ (img/s) | è®­ç»ƒæ—¶é—´/epoch (s) | æœ€ä½³å‡†ç¡®ç‡ (%) |
|------|-------------------|-------------------|--------------|
| Baseline DDP | [ä»ç»“æœå¡«å…¥] | [ä»ç»“æœå¡«å…¥] | [ä»ç»“æœå¡«å…¥] |
| Manual All-Reduce | [ä»ç»“æœå¡«å…¥] | [ä»ç»“æœå¡«å…¥] | [ä»ç»“æœå¡«å…¥] |
| Parameter Server | [ä»ç»“æœå¡«å…¥] | [ä»ç»“æœå¡«å…¥] | [ä»ç»“æœå¡«å…¥] |

2. **æ’å…¥å¯è§†åŒ–å›¾è¡¨:**
   - ååé‡å¯¹æ¯”å›¾ï¼ˆ`throughput_comparison.png`ï¼‰
   - è®­ç»ƒæ—¶é—´å¯¹æ¯”å›¾ï¼ˆ`training_time_comparison.png`ï¼‰
   - Lossæ”¶æ•›æ›²çº¿ï¼ˆ`loss_curves.png`ï¼‰
   - é€šä¿¡å¼€é”€å¯¹æ¯”ï¼ˆ`communication_overhead.png`ï¼‰

3. **æ€§èƒ½åˆ†æè¦ç‚¹:**
   - **ååé‡:** DDPé€šå¸¸æœ€é«˜ï¼ŒPSæœ€ä½ï¼ˆè§£é‡ŠåŸå› ï¼šé€šä¿¡æ¨¡å¼ï¼‰
   - **æ”¶æ•›æ€§:** è§‚å¯ŸLossæ›²çº¿ï¼Œåˆ¤æ–­æ˜¯å¦ä¸€è‡´æ”¶æ•›
   - **é€šä¿¡å¼€é”€:** PSçš„å‚æ•°æ‹‰å–+æ¢¯åº¦æ¨é€ vs All-Reduceçš„åŒæ­¥æ—¶é—´
   - **å¯æ‰©å±•æ€§:** PSåœ¨å¤§è§„æ¨¡æ—¶çš„ç“¶é¢ˆ

4. **ç»“è®º:**
   - DDPæ›´é€‚åˆåŒæ„é›†ç¾¤å’Œé«˜åååœºæ™¯
   - PSé€‚åˆå¼‚æ„ç¯å¢ƒæˆ–éœ€è¦çµæ´»å‚æ•°ç®¡ç†çš„åœºæ™¯
   - é€šä¿¡æ•ˆç‡æ˜¯å…³é”®æ€§èƒ½å·®å¼‚æ¥æº

---

## ğŸ”§ å‚æ•°è°ƒæ•´

å¦‚æœéœ€è¦è°ƒæ•´å®éªŒå‚æ•°ï¼š

### ä¿®æ”¹GPUæ•°é‡
```bash
# ä½¿ç”¨2ä¸ªGPU
torchrun --nproc_per_node=2 baseline_multi_card.py

# ä½¿ç”¨8ä¸ªGPU
torchrun --nproc_per_node=8 baseline_multi_card.py
```

### ä¿®æ”¹Batch Sizeï¼ˆå¦‚æœæ˜¾å­˜ä¸è¶³ï¼‰
```bash
torchrun --nproc_per_node=4 ps_train.py --batch-size 32
```

### ä¿®æ”¹è®­ç»ƒEpochæ•°ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
```bash
torchrun --nproc_per_node=4 ps_train.py --epochs 1
```

### æŒ‡å®šæ•°æ®é›†è·¯å¾„
```bash
torchrun --nproc_per_node=4 ps_train.py --data-dir /path/to/imagenet
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### 1. CUDA Out of Memory
**è§£å†³æ–¹æ¡ˆ:** å‡å°batch size
```bash
torchrun --nproc_per_node=4 ps_train.py --batch-size 32
```

### 2. æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›†
**æ£€æŸ¥æ•°æ®é›†ç»“æ„:**
```bash
ls ./train/ ./val/
```
**æˆ–æŒ‡å®šæ­£ç¡®è·¯å¾„:**
```bash
torchrun --nproc_per_node=4 ps_train.py --data-dir /correct/path/
```

### 3. ç¼ºå°‘ResNet50æƒé‡æ–‡ä»¶
å¦‚æœ `resnet50-0676ba61.pth` ä¸å­˜åœ¨ï¼Œéœ€è¦ä¿®æ”¹è„šæœ¬ä½¿ç”¨åœ¨çº¿ä¸‹è½½ï¼š
- å°†ä¸‰ä¸ªè„šæœ¬ä¸­çš„æƒé‡åŠ è½½éƒ¨åˆ†æ”¹ä¸ºï¼š
```python
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
# åˆ é™¤æˆ–æ³¨é‡Šæ‰ state_dictåŠ è½½ç›¸å…³ä»£ç 
```

### 4. åˆ†æè„šæœ¬æŠ¥é”™ï¼ˆç¼ºå°‘matplotlibï¼‰
```bash
pip install matplotlib numpy
```

---

## ğŸ“¦ å®Œæ•´æ–‡ä»¶æ¸…å•

å®éªŒå‰ç¡®ä¿æœ‰ä»¥ä¸‹æ–‡ä»¶ï¼š
- âœ… `baseline_multi_card.py` - DDPè®­ç»ƒè„šæœ¬
- âœ… `all_reduce_train.py` - All-Reduceè®­ç»ƒè„šæœ¬
- âœ… `ps_train.py` - Parameter Serverè®­ç»ƒè„šæœ¬
- âœ… `analyze_results.py` - æ€§èƒ½åˆ†æè„šæœ¬
- âœ… `run_all_experiments.sh` - è‡ªåŠ¨åŒ–è¿è¡Œè„šæœ¬
- âœ… `EXPERIMENT_GUIDE.md` - è¯¦ç»†å®éªŒæŒ‡å—
- âœ… `README_CN.md` - æœ¬æ–‡ä»¶ï¼ˆå¿«é€Ÿæ“ä½œæŒ‡å—ï¼‰

å®éªŒåç”Ÿæˆçš„æ–‡ä»¶ï¼š
- ğŸ“Š `results_baseline_ddp.json`
- ğŸ“Š `results_all_reduce.json`
- ğŸ“Š `results_ps.json`
- ğŸ“„ `performance_report.txt`
- ğŸ“ `plots/` ç›®å½•åŠå…¶ä¸­çš„5å¼ å›¾è¡¨

---

## ğŸš€ å®Œæ•´å‘½ä»¤é€ŸæŸ¥

```bash
# ä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰
chmod +x run_all_experiments.sh && ./run_all_experiments.sh

# æˆ–æ‰‹åŠ¨è¿è¡Œä¸‰ä¸ªå®éªŒ
torchrun --nproc_per_node=4 baseline_multi_card.py
torchrun --nproc_per_node=4 all_reduce_train.py --epochs 3 --batch-size 64 --workers 16 --data-dir ./ --backend nccl
torchrun --nproc_per_node=4 ps_train.py --epochs 3 --batch-size 64 --workers 16 --data-dir ./ --backend nccl

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
python analyze_results.py

# æŸ¥çœ‹ç»“æœ
cat performance_report.txt
ls -lh plots/
```

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

1. æŸ¥çœ‹è¯¦ç»†æŒ‡å—ï¼š`EXPERIMENT_GUIDE.md`
2. æ£€æŸ¥è„šæœ¬æ—¥å¿—è¾“å‡ºï¼ˆå®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦å’ŒæŒ‡æ ‡ï¼‰
3. ç¡®è®¤ç¯å¢ƒé…ç½®ï¼ˆGPUã€æ•°æ®é›†ã€ä¾èµ–åŒ…ï¼‰

**ç¥å®éªŒé¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜å¯éšæ—¶æŸ¥é˜…æ–‡æ¡£ã€‚** ğŸ‰
